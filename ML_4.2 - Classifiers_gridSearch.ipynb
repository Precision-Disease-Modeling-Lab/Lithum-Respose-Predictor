{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df50fb9-d00a-4109-81c6-fa0523d73a2d",
   "metadata": {},
   "source": [
    "# Lithium Response predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2ad97-f7fb-43de-83fa-e6f533a26d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b41baea-5f82-4f3f-8cf2-2173750c4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afcf44d9-29ef-42c9-aeb2-ed3eb3deccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ddd38-f6b8-434f-9125-d858b1eab6b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9790d5-0d1c-4c1b-83ee-b3ebf0faf274",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False : \n",
    "    # import samples table\n",
    "    samples_df = pd.read_csv(r'../data/dataset_joined/samples_joined.csv', index_col=0)\n",
    "    samples_df.drop(columns=['patient', 'sex','age'], inplace=True)\n",
    "    # import count matrix\n",
    "    countMatrix_df = pd.read_csv('../data/dataset_joined/countMatrix_include_gene_name.csv', index_col=0).T\n",
    "    df_full =  countMatrix_df.merge(samples_df, left_index=True, right_index= True)\n",
    "    \n",
    "    #import list of genes from deseq2 analysis\n",
    "    genes = pd.read_csv('vsd_results_FC1.csv', index_col=0).T.columns\n",
    "\n",
    "    # keep only relevant dataset, i.e. remove CTRL samples\n",
    "    df = df_full[df_full['diagnosis']!= 'ctrl']\n",
    "\n",
    "    ##  filter only relevant genes from deseq2 analysis ##\n",
    "    df = df.loc[:,list(genes) + ['batch','condition']]\n",
    "    df['condition']=df['condition'].apply(lambda x : 1 if x =='LR' else 0)\n",
    "    \n",
    "    df.to_csv('LiResp_dataset.csv')\n",
    "\n",
    "if False : \n",
    "    # import samples table\n",
    "    samples_df = pd.read_csv(r'samples_joined.csv', index_col=0)\n",
    "    samples_df.drop(columns=['patient', 'sex','age'], inplace=True)\n",
    "    # import count matrix\n",
    "    countMatrix_df = pd.read_csv('countMatrix_include_gene_name.csv', index_col=0).T\n",
    "    df_full =  countMatrix_df.merge(samples_df, left_index=True, right_index= True)\n",
    "    \n",
    "    #import list of genes from deseq2 analysis\n",
    "    genes = pd.read_csv('original_dataset_CTRL_vs_BD_counts_results_FC1.csv', index_col=0).T.columns\n",
    "\n",
    "    # keep only relevant dataset, i.e. drop batch #4 becuase it has no control\n",
    "    df = df_full[df_full['batch']!= 4]\n",
    "\n",
    "    ##  filter only relevant genes from deseq2 analysis ##\n",
    "    df = df.loc[:,list(genes) + ['diagnosis']]\n",
    "    df['diagnosis'] = df['diagnosis'].apply(lambda x : 1 if x =='BD' else 0)\n",
    "    df.to_csv('BP_vsCtrl_dataset.csv')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d5ca7d-ad6c-49ea-8eb9-4fc8548ffe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('LiResp_dataset.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b09e004-6e4b-4f58-880b-fb68d150d61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb3ab89b-464a-43fb-ae0f-b7ff0d807d6e",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e81af01e-5d88-4ea7-b57b-6b905ab8bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers=[\n",
    "    {'name': \"SVM\", \n",
    "     'model': SVC(max_iter=1000, probability=True), \n",
    "     'cvGridParams' :[\n",
    "                      {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "                      {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "                     ]\n",
    "    },\n",
    "                       \n",
    "    {'name' : \"Neural Net\", \n",
    "     'model': MLPClassifier(max_iter = 5000,  verbose=True,), \n",
    "    'cvGridParams' : {\n",
    "                      'alpha' : 10. ** np.arange(-3, 2), \n",
    "                      'activation' : ['tanh', 'relu'],\n",
    "                      'hidden_layer_sizes' : [(x,) for x in [50,150,300,500,700]], \n",
    "                      'learning_rate' : ['constant', 'adaptive']\n",
    "                     }\n",
    "    \n",
    "    }, \n",
    "    {'name' : \"Naive Bayes\", \n",
    "     'model':GaussianNB(),\n",
    "     'cvGridParams' :  {'var_smoothing': np.logspace(0,-9, num=100)}}, \n",
    "    \n",
    "    {'name' : \"Random Forest\", \n",
    "     'model': RandomForestClassifier(), \n",
    "     'cvGridParams' :  {'n_estimators':[int(x) for x in np.linspace(10,1000,10)],\n",
    "                        'criterion': [\"gini\", \"entropy\"]}\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e1df0f-a92b-49fe-b275-ad6a4f2687ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916316b0-2bc5-4fd8-9133-4af8a78a2acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dd5bf3c-40c0-469e-9e38-85f83e8684bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(columns=['batch','condition']))\n",
    "y = np.array(df['condition'].apply(lambda x : 1 if x =='LR' else 0))\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)\n",
    "# cv = ShuffleSplit(n_splits=20, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4ad6e-db01-40a0-94e0-902d831f2c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62be69c6-472c-4fb4-a087-22fcb273b74d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "293b89dd-f542-4b7b-8ca8-55a224ee7a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "{'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "Neural Net\n",
      "Iteration 1, loss = 0.68839473\n",
      "Iteration 2, loss = 0.43935347\n",
      "Iteration 3, loss = 0.28036670\n",
      "Iteration 4, loss = 0.18311312\n",
      "Iteration 5, loss = 0.12291276\n",
      "Iteration 6, loss = 0.08463639\n",
      "Iteration 7, loss = 0.05989003\n",
      "Iteration 8, loss = 0.04369883\n",
      "Iteration 9, loss = 0.03291890\n",
      "Iteration 10, loss = 0.02556443\n",
      "Iteration 11, loss = 0.02040692\n",
      "Iteration 12, loss = 0.01669114\n",
      "Iteration 13, loss = 0.01394732\n",
      "Iteration 14, loss = 0.01187656\n",
      "Iteration 15, loss = 0.01028354\n",
      "Iteration 16, loss = 0.00903716\n",
      "Iteration 17, loss = 0.00804727\n",
      "Iteration 18, loss = 0.00725044\n",
      "Iteration 19, loss = 0.00660121\n",
      "Iteration 20, loss = 0.00606637\n",
      "Iteration 21, loss = 0.00562129\n",
      "Iteration 22, loss = 0.00524748\n",
      "Iteration 23, loss = 0.00493083\n",
      "Iteration 24, loss = 0.00466048\n",
      "Iteration 25, loss = 0.00442796\n",
      "Iteration 26, loss = 0.00422662\n",
      "Iteration 27, loss = 0.00405117\n",
      "Iteration 28, loss = 0.00389738\n",
      "Iteration 29, loss = 0.00376183\n",
      "Iteration 30, loss = 0.00364173\n",
      "Iteration 31, loss = 0.00353480\n",
      "Iteration 32, loss = 0.00343914\n",
      "Iteration 33, loss = 0.00335319\n",
      "Iteration 34, loss = 0.00327565\n",
      "Iteration 35, loss = 0.00320540\n",
      "Iteration 36, loss = 0.00314152\n",
      "Iteration 37, loss = 0.00308323\n",
      "Iteration 38, loss = 0.00302983\n",
      "Iteration 39, loss = 0.00298076\n",
      "Iteration 40, loss = 0.00293552\n",
      "Iteration 41, loss = 0.00289368\n",
      "Iteration 42, loss = 0.00285486\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "{'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive'}\n",
      "Naive Bayes\n",
      "{'var_smoothing': 0.15199110829529336}\n",
      "Random Forest\n",
      "{'criterion': 'gini', 'n_estimators': 120}\n"
     ]
    }
   ],
   "source": [
    "for clf in classifiers: \n",
    "    print(clf['name'])\n",
    "    clf['gridSearch'] = GridSearchCV(clf['model'], clf['cvGridParams'], n_jobs=2, cv=5)\n",
    "   \n",
    "    clf['gridSearch'].fit(X_train, y_train)\n",
    "    clf['best_params_'] = clf['gridSearch'].best_params_\n",
    "    \n",
    "    clf['score'] =  clf['gridSearch'].score(X_test, y_test)\n",
    "    print(clf['best_params_'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1920911-7126-4e06-b91d-c7942e581197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(classifiers, open( \"classifiers_gridSearch.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e4ba833-2c3d-41e6-816d-99b9a1a14007",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = pickle.load( open( \"classifiers_gridSearch.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e58e05-4d5a-4bf4-a71a-4a5b4ed62136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc91bcf3-435f-459f-bc37-7811168a0f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
